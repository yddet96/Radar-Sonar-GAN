# -*- coding: utf-8 -*-
"""radar_gan_low.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cS742ej4Up1opnLU7Sz-bl8uy-qShbfd
"""

import tensorflow as tf
import numpy as np
from matplotlib import pyplot as plt
# import pickle
import os
from google.colab import files
from google.colab import drive

def Generator(z, g_vars, batch_size, x_sz = 1547520):
  '''
  Define generator architecture
  '''
  H1 = tf.nn.leaky_relu( tf.matmul(z, g_vars[0]) + g_vars[1] )
  Z = tf.reshape(H1, shape = [batch_size, 355,12,1])
  
  Cg1 = tf.nn.conv2d_transpose(Z, g_vars[2], output_shape = [batch_size, 358, 15, 16], strides = [1,1,1,1], padding = "VALID") + g_vars[3]
  Ag1 = tf.nn.leaky_relu(Cg1)
  
  Cg2 = tf.nn.conv2d_transpose(Ag1, g_vars[4], output_shape = [batch_size, 373, 30, 32], strides = [1,1,1,1], padding = "VALID") + g_vars[5]
  Ag2 = tf.nn.leaky_relu(Cg2)
    
  Cg3 = tf.nn.conv2d_transpose(Ag2, g_vars[6], output_shape = [batch_size, 388, 45, 64], strides = [1,1,1,1], padding = "VALID") + g_vars[7]
  Ag3 = tf.nn.leaky_relu(Cg3)
  
  Cg4 = tf.nn.conv2d_transpose(Ag3, g_vars[8], output_shape = [batch_size, 403, 60, 128], strides = [1,1,1,1], padding = "VALID") + g_vars[9]
  gen = tf.reshape(Cg4, shape = [batch_size, x_sz, 2])

  return(gen)


def Discriminator(x, d_vars, batch_size, pool_sz = 4, p_len = 6048):
  '''
  Define discriminator architecture
  '''
  X = tf.reshape(x, shape = [batch_size, 1860, 1664, 1])

  Cd1 = tf.nn.conv2d(X, d_vars[0], strides = [1,1,1,1], padding = "VALID") + d_vars[1]
  Ad1 = tf.nn.leaky_relu(Cd1)
  Pd1 = tf.nn.max_pool(Ad1, ksize = (1,pool_sz,pool_sz,1), strides = (1,pool_sz,pool_sz,1), padding = "VALID")

  Cd2 = tf.nn.conv2d(Pd1, d_vars[2], strides = [1,1,1,1], padding = "VALID") + d_vars[3]
  Ad2 = tf.nn.leaky_relu(Cd2)
  Pd2 = tf.nn.max_pool(Ad2, ksize = (1,pool_sz,pool_sz,1), strides = (1,pool_sz,pool_sz,1), padding = "VALID")
  
  Cd3 = tf.nn.conv2d(Pd2, d_vars[4], strides = [1,1,1,1], padding = "VALID") + d_vars[5]
  Ad3 = tf.nn.leaky_relu(Cd3)
  Pd3 = tf.nn.max_pool(Ad3, ksize = (1,pool_sz,pool_sz,1), strides = (1,pool_sz,pool_sz,1), padding = "VALID")
    
  P = tf.reshape(Pd3, shape = [batch_size, p_len])
  H = tf.nn.leaky_relu( tf.matmul(P, d_vars[6]) + d_vars[7] )
  out = tf.nn.sigmoid( tf.matmul(H, d_vars[8]) + d_vars[9] )
  
  return(out)



def main():
  drive.mount("/content/gdrive")
  print("Initializing...")

  '''
  Load data.
  '''
  print("\tloading data")
  data = np.load("/content/gdrive/My Drive/MS Project/norm_data.npy", mmap_mode = "r")
  
  print("\tinitializing network")
  x_sz = 1547520
  z_init = 100
  z_sz = 355*12
  M = 500

  std = 0.001
  d_filters = [4,8,16]
  batch_size = 20
  p_len = 378*d_filters[-1]

  g_lr_init = 0.001
  d_lr_init = 0.01

  epochs = 3


  '''
  Define placeholders
  '''
  z = tf.placeholder( tf.float32, shape = (batch_size, z_init) )
  x = tf.placeholder( tf.float32, shape = (batch_size, x_sz, 2) )

  '''
  Define generator parameters
  '''
#   with tf.variable_scope("GAN/Generator"):
  Wgi = tf.Variable( tf.random_normal(shape = (z_init, z_sz), dtype = tf.float32, stddev = std) )
  bgi = tf.Variable( tf.zeros(shape = (z_sz), dtype = tf.float32) )

  Wg1 = tf.Variable( tf.random_normal(shape = (4,4,16,1), dtype = tf.float32, stddev = std) )
  Wg2 = tf.Variable( tf.random_normal(shape = (16,16,32,16), dtype = tf.float32, stddev = std) )
  Wg3 = tf.Variable( tf.random_normal(shape = (16,16,64,32), dtype = tf.float32, stddev = std) )
  Wg4 = tf.Variable( tf.random_normal(shape = (16,16,128,64), dtype = tf.float32, stddev = std) )

  bg1 = tf.Variable( tf.zeros(shape = (16), dtype = tf.float32) )
  bg2 = tf.Variable( tf.zeros(shape = (32), dtype = tf.float32) )
  bg3 = tf.Variable( tf.zeros(shape = (64), dtype = tf.float32) )
  bg4 = tf.Variable( tf.zeros(shape = (128), dtype = tf.float32) )
   
  g_vars = [Wgi, bgi, Wg1, bg1, Wg2, bg2, Wg3, bg3, Wg4, bg4]

  '''
  Define discriminator parameters
  '''
  
#   with tf.variable_scope("GAN/Discriminator"):
  Wd1 = tf.Variable( tf.random_normal(shape = (23,23,1,d_filters[0]), dtype = tf.float32, stddev = std) )
  Wd2 = tf.Variable( tf.random_normal(shape = (23,23,d_filters[0],d_filters[1]), dtype = tf.float32, stddev = std) )
  Wd3 = tf.Variable( tf.random_normal(shape = (23,23,d_filters[1],d_filters[2]), dtype = tf.float32, stddev = std) )

  bd1 = tf.Variable( tf.zeros(shape = (d_filters[0]), dtype = tf.float32) )
  bd2 = tf.Variable( tf.zeros(shape = (d_filters[1]), dtype = tf.float32) )
  bd3 = tf.Variable( tf.zeros(shape = (d_filters[2]), dtype = tf.float32) )

  Wdf = tf.Variable( tf.random_normal(shape = (p_len,100), dtype = tf.float32, stddev = std) )
  bdf = tf.Variable( tf.zeros(shape = (100), dtype = tf.float32) )

  Wdo = tf.Variable( tf.random_normal(shape = (100,1), dtype = tf.float32, stddev = std) )
  bdo = tf.Variable ( tf.zeros(shape = (1), dtype = tf.float32) )
  
  d_vars = [Wd1, bd1, Wd2, bd2, Wd3, bd3, Wdf, bdf, Wdo, bdo]
  pool_sz = 4 
    
  '''
  Retrieve network variables for each genrator and discriminator
  '''
#   gen_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="GAN/Generator")
#   disc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="GAN/Discriminator")
  
  '''
  Define GAN
  '''
  gen_sample = Generator(z, g_vars, batch_size)

  r_dec = Discriminator(x, d_vars, batch_size, p_len = p_len)
  f_dec = Discriminator(gen_sample, d_vars, batch_size, p_len = p_len)
  
  '''
  Define loss function
  '''
  disc_loss = tf.nn.sigmoid_cross_entropy_with_logits( logits=r_dec, labels=tf.ones_like(r_dec) ) \
            + tf.nn.sigmoid_cross_entropy_with_logits( logits=f_dec, labels=tf.zeros_like(f_dec) )
  disc_loss = tf.reduce_mean(disc_loss)

  gen_loss = tf.nn.sigmoid_cross_entropy_with_logits( logits=f_dec, labels=tf.ones_like(f_dec) )
  gen_loss = tf.reduce_mean(gen_loss)

#   disc_loss = -tf.log(r_dec) - tf.log( tf.ones_like(f_dec) - f_dec)
#   disc_loss = tf.reduce_mean(disc_loss)
  
#   gen_loss = -tf.log(f_dec)
#   gen_loss = tf.reduce_mean(gen_loss)
  
  '''
  Define optimizers
  '''
  gen_step = tf.train.AdamOptimizer(learning_rate = g_lr_init).minimize(gen_loss, var_list = g_vars)
  disc_step = tf.train.AdamOptimizer(learning_rate = d_lr_init).minimize(disc_loss, var_list = d_vars)
  
  '''
  Save model.
  '''
  # Save the model
  tf.get_collection('generator_nodes')

  # Add opts to the collection
  tf.add_to_collection('generator_nodes', z)
  tf.add_to_collection('generator_nodes', gen_sample)

  saver = tf.train.Saver()

  '''
  Train network.
  '''
  print("Training...")
  init = tf.global_variables_initializer()
  idx_list = np.arange(M)
  
  g_loss_list = []
  d_loss_list = []

  with tf.Session() as sess:
    sess.run(init)

    for e in range(epochs):
      '''
      For each epoch, randomize the order of samples to choose.
      '''
      print("    Epoch %2d" % e)
      np.random.shuffle(idx_list)

      for b in range(M//batch_size):
        '''
        Select each minibatch and run increment the network parameters.
        '''
        batch_idx = idx_list[b*batch_size : (b+1)*batch_size]
        x_batch = data[batch_idx, :,:]
        z_batch = np.random.random((batch_size, z_init))

        d_loss, _, r_test, f_test = sess.run([disc_loss, disc_step, r_dec, f_dec], feed_dict = {x: x_batch, z: z_batch})
#         g_loss, _ = sess.run([gen_loss, gen_step], feed_dict = {z: z_batch})
#         d_loss, g_loss, _, _, r_test, f_test = sess.run([disc_loss, gen_loss, disc_step, gen_step, r_dec, f_dec], feed_dict = {x: x_batch, z: z_batch} )

#         print("\tminibatch %2d ==> d_loss: %f   g_loss: %f" % (b, d_loss, g_loss))
        print("\tminibatch %2d ==> d_loss: %f " % (b, d_loss))
        print("r", r_test.T)
        print("f", f_test.T)

#         g_loss_list.append(g_loss)
        d_loss_list.append(d_loss)
    
      '''
      At the end of each epoch, show the loss plots
      '''
      plt.figure(1)
#       plt.plot(g_loss_list)
      plt.plot(d_loss_list)
      plt.legend(["Generator Loss","Discriminator Loss"])
      plt.show()
      
    '''
    Save Model
    '''
    save_path = saver.save(sess,"/content/gdrive/My Drive/MS Project/radar_gan")
    
    '''
    Plot loss
    '''    
    plt.figure(1)
#     plt.plot(g_loss_list)
    plt.plot(d_loss_list)
    plt.legend(["Generator Loss","Discriminator Loss"])
    plt.savefig("/content/gdrive/My Drive/MS Project/loss.png")

  return()



if __name__ == '__main__':
  main()